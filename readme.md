I'm a research associate (postdoc) in the School of Engineering at Cardiff University.
I'm excited about enabling robots to manipulate real-world objects, rigid or deformable, through learning or non-learning methods.
I studied deep (hierarchical) reinforcement learning and affordance learning for rigid object grasping and manipulation as a PhD student at Cardiff University and received the degree in October 2023.
Now I'm developing methods to simulate and handle real-world deformable and granular objects.
I enjoy deeply working on robots, seeing them performing tasks for humans, and of course, understanding why they are not doing the right things!!
I also devote myself to open-source software development - knowledge should be shared.
Before my PhD, I received my Bachelor's and Master's degrees in Mechanical and Industrial Engineering from the Guangdong University of Technology in China.

#### Reach out!

<a href="mailto:YangX66@cardiff.ac.uk">
   <img src="https://techcommunity.microsoft.com/t5/image/serverpage/image-id/172206i70472167E79B9D0F/image-size/large?v=v2&px=999" width="30" height="30">
</a>
<a href="https://scholar.google.com/citations?user=pJoieqMAAAAJ">
   <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Google_Scholar_logo.svg/2048px-Google_Scholar_logo.svg.png" width="30" height="30">
</a>
<a href="https://www.zhihu.com/people/xiao-yang-69-78-27/">
   <img src="https://picx.zhimg.com/v2-4cd83ae3d6ca76dabecf001244a62310.jpg?source=57bbeac9" width="30" height="30">
</a>
<a href="https://www.linkedin.com/in/xintong-yang-016a9a250/">
   <img src="https://www.edigitalagency.com.au/wp-content/uploads/Linkedin-logo-icon-png.png" width="30" height="30">
</a>

#### News
- [2025 Jan.] I have been recruited by a new BBSRC project as a research associate at Cardiff University, working on the development of an automatic/robotised biology lab.<a href="https://gtr.ukri.org/projects?ref=BB%2FY008537%2F1">
             <img src="https://gtr.ukri.org/resources/img/ukrilogo.png?" width="15" height="15">
- [2025 Jan.] Our new preprent titled "Differentiable Physics-based System Identification for Robotic Manipulation of Elastoplastic Materials" is now online<a href="https://arxiv.org/abs/2411.00554">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>, with video demo <a href="https://www.youtube.com/watch?v=2-9JWRsQhTU">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/2560px-YouTube_full-color_icon_%282017%29.svg.png" width="20" height="15">
             </a>.
- [2025 Jan.] Our new preprent titled "AutomaChef: A Physics-informed Demonstration-guided Learning Framework for Granular Material Manipulation" is now online<a href="https://arxiv.org/abs/2406.09178">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>
- [2024 Jan.] Our new preprint titled "Gam: General Affordance-Based Manipulation for Contact-Rich Object Disentangling Tasks" is now online<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4606046">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>, with video demo <a href="https://www.youtube.com/watch?v=Rao_Ctfh9BI">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/2560px-YouTube_full-color_icon_%282017%29.svg.png" width="20" height="15">
             </a>.

#### Representative Publications
<table>
  <tbody>
   <tr>
      <td>2025</td>
      <td><b>Yang, X.</b>, Lai Y., Ji Z.Differentiable Physics-based System Identification for Robotic Manipulation of Elastoplastic Materials. IJRR (under review).
          (Preprint <a href="https://arxiv.org/abs/2411.00554">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>) <a href="https://www.youtube.com/watch?v=2-9JWRsQhTU">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/2560px-YouTube_full-color_icon_%282017%29.svg.png" width="20" height="15">
             </a></td>

   </tr>
   <tr>
      <td>2024</td>
      <td><b>Yang, X.</b>, Wu J., Lai Y., Ji Z. Gam: General Affordance-Based Manipulation for Contact-Rich Object Disentangling Tasks. Neurocomputing.
          (Preprint <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4606046">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>) <a href="https://www.youtube.com/watch?v=Rao_Ctfh9BI">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/2560px-YouTube_full-color_icon_%282017%29.svg.png" width="20" height="15">
             </a></td>

   </tr>
   <tr>
      <td>2023</td>
      <td><b>Yang, X.</b> Robotic Manipulation via Hierarchical and Affordance Learning. PhD thesis.
          <a href="https://orca.cardiff.ac.uk/id/eprint/162467">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a></td>

   </tr>
    <tr>
      <td>2023 </td>
      <td><b>Yang, X.</b>, Ji, Z., Wu, J., Lai, Y. Recent Advances of Deep Robotic Affordance Learning: A Reinforcement Learning Perspective. 
             IEEE TCDS.
          (Preprint <a href="https://arxiv.org/abs/2303.05344">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>)
       </td>
    </tr>
    <tr>
      <td>2021</td>
      <td><b>Yang, X.</b>, Ji, Z., Wu, J., Lai, Y. An Open-Source Multi-Goal Reinforcement Learning Environment for Robotic Manipulation with Pybullet. TAROS. 
          <a href="https://arxiv.org/abs/2105.05985">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>
          <a href="https://github.com/IanYangChina/pybullet_multigoal_gym">
             <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" width="15" height="15">
             </a></td>
    </tr>
    <tr>
      <td>2021</td>
      <td><b>Yang, X.</b>, Ji, Z., Wu, J., Lai, Y., Wei, C., Liu, G. & Setchi, R. Hierarchical Reinforcement Learning with Universal Policies for Multi-Step Robotic Manipulation. IEEE TNNLS.
          <a href="https://ieeexplore.ieee.org/document/9366328">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/PDF_icon.svg/1200px-PDF_icon.svg.png" width="15" height="15">
             </a>
          <a href="https://www.youtube.com/watch?v=n_wQuf4r0qk">
             <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/2560px-YouTube_full-color_icon_%282017%29.svg.png" width="20" height="15">
             </a>
          <a href="https://github.com/IanYangChina/UOF-paper-code">
             <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" width="15" height="15">
             </a></td>
    </tr>
  </tbody>
</table>
